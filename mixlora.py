# -*- coding: utf-8 -*-
"""MixLoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NYAsLKZ6jCId-u9UKPmGxak_TnRkoWb2
"""

pip install -q torch transformers peft datasets accelerate

import torch
import torch.nn as nn
import torch.optim as optim

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, PeftModel

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True
).to(device)

base_model.config.pad_token_id = tokenizer.eos_token_id
base_model.eval()

lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

sentiment_data = [
    "The product was amazing and I loved it.",
    "This was the worst experience ever.",
]

summary_data = [
    "Summarize: The movie had great visuals but weak story.",
    "Summarize: The phone has good battery but poor camera.",
]

def train_lora_adapter(base_model, texts, save_name):
    model = get_peft_model(base_model, lora_config)
    model.train()

    optimizer = optim.AdamW(model.parameters(), lr=5e-4)

    for epoch in range(2):  # very small training
        for text in texts:
            inputs = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=128
            ).to(device)

            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    model.save_pretrained(save_name)
    print(f"Saved adapter: {save_name}")

train_lora_adapter(base_model, sentiment_data, "lora_sentiment")
train_lora_adapter(base_model, summary_data, "lora_summary")

model = PeftModel.from_pretrained(
    base_model,
    "lora_sentiment",
    adapter_name="sentiment"
)

model.load_adapter("lora_summary", adapter_name="summary")
# model.set_adapter(["sentiment", "summary"]) # This line caused the error
model.eval()

class GatingNetwork(nn.Module):
    def __init__(self, hidden_size, num_adapters):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, num_adapters)
        )

    def forward(self, x):
        return torch.softmax(self.net(x), dim=-1)

def get_prompt_embedding(model, input_ids, attention_mask):
    with torch.no_grad():
        outputs = model.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        )
    return outputs.hidden_states[-1].mean(dim=1)

adapter_names = ["sentiment", "summary"]
hidden_size = model.config.hidden_size

gating_net = GatingNetwork(hidden_size, len(adapter_names)).to(device)

def sparse_top1(weights):
    idx = torch.argmax(weights)
    sparse = torch.zeros_like(weights)
    sparse[idx] = 1.0
    return sparse

def apply_mixlora(prompt):
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=256
    ).to(device)

    embedding = get_prompt_embedding(
        model,
        inputs["input_ids"],
        inputs["attention_mask"]
    )

    weights = gating_net(embedding)[0]
    sparse_weights = sparse_top1(weights)

    # Use set_adapter to activate the single adapter with non-zero weight
    activated_adapter_index = torch.argmax(sparse_weights).item()
    model.set_adapter(adapter_names[activated_adapter_index])

    return inputs, sparse_weights

routing_data = [
    ("The product was terrible", 0),   # sentiment
    ("Summarize this review", 1),       # summary
]

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(gating_net.parameters(), lr=1e-3)

gating_net.train()

for epoch in range(10):
    total_loss = 0
    for text, label in routing_data:
        inputs = tokenizer(text, return_tensors="pt").to(device)

        embedding = get_prompt_embedding(
            model,
            inputs["input_ids"],
            inputs["attention_mask"]
        )

        preds = gating_net(embedding)
        loss = criterion(preds, torch.tensor([label]).to(device))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} | Loss: {total_loss:.4f}")

prompt = "Summarize the emotional tone of this review: The service was slow but polite."

inputs, weights = apply_mixlora(prompt)

print("Adapter Weights:")
for i, name in enumerate(adapter_names):
    print(f"{name}: {weights[i].item():.2f}")

outputs = model.generate(
    **inputs,
    max_new_tokens=60
)

print("\nModel Output:\n")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

